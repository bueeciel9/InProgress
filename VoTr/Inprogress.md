* Summary

- Conventional 3D convolutional backbones in voxel-based 3D detectors can't efficiently capture large context information.
- Transformer-based architecture that enables long-range relationships between voxels by self-attention.

Sparse voxel module & submanifold voxel module -> can operate on the empty and non-empty voxel positions effectively.

Local Attention & Dilated Attention

Fast Voxel Query -> to accelerate the querying process in multi-head attention.


![image](https://user-images.githubusercontent.com/65759092/183721463-a25aa9e0-1cde-4403-a7f9-7195352b2df4.png)

