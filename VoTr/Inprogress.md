- Conventional 3D convolutional backbones in voxel-based 3D detectors can't efficiently capture large context information.
- Transformer-based architecture that enables long-range relationships between voxels by self-attention.

Sparse voxel module & submanifold voxel module -> can operate on the empty and non-empty voxel positions effectively.

Local Attention & Dilated Attention

Fast Voxel Query -> to accelerate the querying process in multi-head attention.



